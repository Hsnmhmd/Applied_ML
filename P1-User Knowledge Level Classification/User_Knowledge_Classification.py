# -*- coding: utf-8 -*-
"""Group17_HW1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CejSu-ixnstitOMq3MM2J8riSsT_Zw7w
"""

#importing libraries
import numpy as np #for math
import matplotlib.pyplot as plt #to plot
import pandas as pd #to import datasets and manage them
from sklearn import preprocessing
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score
from sklearn.linear_model import Perceptron
#feature selection
from sklearn.feature_selection import SelectKBest, f_classif

def plot_cm(X,y,model,title):
  predictions = model.predict(X)
  cm = confusion_matrix(y, predictions, labels=model.classes_)
  disp = ConfusionMatrixDisplay(confusion_matrix=cm,
                                display_labels=model.classes_)
  disp.plot()
  plt.title(title)
  plt.show() 

def plot_cm_for_argmax(y_actual,y_predicted,labels,title):
  predictions = y_predicted
  cm = confusion_matrix(y_actual, predictions, labels=labels)
  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)
  disp.plot()
  plt.title(title)
  plt.show()

def plot_data_for_argmax(X,y,xlabel,ylabel,title):
 
  plt.scatter(X[:,0][(y == 3) ], X[:,1][(y == 3) ], marker='D', color='red', 
              label='Very Low')
  plt.scatter(X[:,0][(y == 2) ], X[:,1][(y == 2) ], marker='o', color='blue',
              label='Medium')
  plt.scatter(X[:,0][(y == 1) ], X[:,1][(y == 1) ], marker='+', color='green',
              label='Low')
  plt.scatter(X[:,0][(y == 0) ], X[:,1][(y == 0) ], marker='*', color='yellow',
              label='High')
  plt.xlabel(xlabel)
  plt.ylabel(ylabel)
  plt.legend()
  plt.title(title)
  plt.show()


def plot_boundries(X,y_original,model,xlabel,ylabel,title):
  x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5
  y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5
  h = 0.02  # step size in the mesh
  xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
  Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
  # Put the result into a color plot for different regions
  Z = Z.reshape(xx.shape)
  plt.figure(3, figsize=(4,3 ))
  plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)
  # Plot also the points
  plt.scatter(X[:, 0], X[:, 1],c=y_original, edgecolors="k", cmap=plt.cm.Paired)
  plt.xlim(xx.min(), xx.max())
  plt.ylim(yy.min(), yy.max())
  plt.xlabel(xlabel)
  plt.ylabel(ylabel)
  plt.title(title)
  plt.show() 

def one_vs_rest(xtrain,xtest,ytrain,ytest,one ):
  X_train_ovr = xtrain
  y_train_ovr = ytrain 
  X_test_ovr = xtest
  y_test_ovr = ytest
  ##----------------
  ##Label Binarizer For the Traing And Testing Labels
  ##----------------
  lb=preprocessing.LabelBinarizer()
  lb.fit([0,1,2,3])
  print("The Original Training Data Of ",one ,"vs Rest \n" ,y_train_ovr )
  y_train_b=lb.transform(y_train_ovr)
  y_ovr = y_train_b[:,one]
  print("The Binarized Labels of Training Data Of ",one ,"vs Rest \n" ,y_ovr )
  
  y_test_b=lb.transform(y_test_ovr)
  y_test_ovr = y_test_b[:,one]

  ####--------------------
  ####Note That The Probability Value Has to be true for the clf.predict_proba() work later
  ####--------------------
  clf = make_pipeline( StandardScaler(),SVC(gamma='auto', probability= True))
  clf.fit(X_train_ovr, y_ovr)
  #####-------------------
  #### Decision Boundry Plotting for Each OVR  
  print(one , "vs Rest SVM Accuracy =",clf.score(X_test_ovr, y_test_ovr))
  plot_boundries(X_test_ovr,y_test_ovr,clf,xlabel="LPR",ylabel="PEG",
                 title=("Decision Boundries for ",one , "vs  Rest" ))  
  plt.show() 
  ####----------------
  ####clf.predict_proba() Returns A List Containing The Confidence scores
  ####For the binary Classifier it returns 2 confidence score for each point 
  ####The Confidence of it being a zero and the confidence of it being one
  ####We Only Care About the confidence score for the ones because we do not know 
  ####The Truth About the zero whether it is 0 or 1 or 2 or 3
  ####----------------
  return clf.predict_proba(X_test_ovr)[ : , 1 ].reshape(-1 , 1)

def one_vs_one_RETURN_PROBABILITY_FOR_ALL(xtrain,xtest,ytrain,ytest,one,vs ):
  X_train = xtrain
  y_train = ytrain
  X_train_ovo = X_train[(y_train == one)|(y_train == vs) ]
  y_train_ovo = y_train[(y_train == one)|(y_train == vs) ]
  X_test = xtest
  y_test = ytest
  X_test_ovo = X_test[(y_test == one)|(y_test == vs) ]
  y_test_ovo = y_test[(y_test == one)|(y_test == vs) ]

  ##----------------
  ##Label Binarizer For the Traing And Testing Labels
  ##----------------
  ##print(y_train_ovo)
  lb=preprocessing.LabelBinarizer()
  lb.fit([one,vs])
  print("The Original Training Data Of ",one ,"vs", vs ,"\n" ,
        y_train_ovo.reshape(1,-1) )
  y_train_b=lb.transform(y_train_ovo)
  print("The Binarized TRaining Labels Of ",one ,"vs", vs ,"\n" ,
        y_train_b.reshape(1,-1) )
  y_ovo = y_train_b[:,0]
  y_test_b=lb.transform(y_test_ovo)
  y_test_ovo = y_test_b[:,0]

  ####--------------------
  ####SVM
  ####--------------------
  ####Note That The Probability Value Has to be true for the clf.predict_proba() work later
  ####--------------------
  clf = make_pipeline( StandardScaler(),SVC(gamma='auto', probability= True))
  clf.fit(X_train_ovo, y_ovo)
  ####---------------
  #### Accuracy and Decision Boundry Plotting for Each OVO
  print(one , "vs" , vs , 
        "SVM Accuracy On The Test Data That Only Belong to The Two Classes=",
        clf.score(X_test_ovo, y_test_ovo))

  plot_boundries(X_test_ovo,y_test_ovo,clf,xlabel="LPR",ylabel="PEG",
       title=("Decision Boundries for Test Data Only In the Two Classes",
                        one , "vs" , vs ))  
  #### Decision Boundry Plotting for Each OVO for the whole Test Data
  plot_boundries(X_test,y_test,clf,xlabel="LPR",ylabel="PEG",
      title=("Decision Boundries for The Whole Test Data For the Classifeir of",
                        one , "vs" , vs )) 
  ####----------------

  ####The associated Confidence scores
  ####For the binary Classifier it returns 2 confidence score for each point IN THE X_test
  ####The Confidence of it being a zero and the confidence of it being one
  ####----------------

  return clf.predict_proba(X_test)[ : , 0:2 ].reshape(-1 , 2)

#importing datasets from google drive
from google.colab import drive
drive.mount('/content/drive/')
data_train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/dataset/DUMD_train.csv')
data_test = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/dataset/DUMD_test.csv')

###------------------
### Problem 1
### Choose two features and Explain how and why you chose the two features?
"""
3 and 4 have the highest variance variance of column 3 = 0.067518 variance of column 4 = 0.071804
"""
#best features selection
X_train = np.array(data_train.iloc[:,:5])
y_train = np.array(data_train.iloc[:,5])
X_test= np.array(data_test.iloc[ : , :5 ])
y_test= np.array(data_test.iloc[ : , 5 ])

selector = SelectKBest(f_classif, k=2)
selector.fit(X_train, y_train)
scores = -np.log10(selector.pvalues_)
scores /= scores.max()

#graphical figure to illustrate the resultant difference in score between the features and the label
X_indices = np.arange(X_train.shape[-1])
plt.figure(1)
plt.bar(X_indices - 0.05, scores, width=0.3)
plt.title("Feature univariate score")
plt.xlabel("Feature number")
plt.ylabel(r"Univariate score ($-Log(p_{value})$)")
plt.show()
#change the training set to contain the 2 best features only
X_train = selector.transform(X_train)
X_test = selector.transform(X_test)
y_train = np.array(data_train.iloc[:, 5]) 
y_test= np.array(data_test.iloc[ : , 5 ])

##convert categorical class labels under the ”UNS” column to numerical values by using the LabelEncoder.
##Label Encoding For Data Training and Testing
##-------------
le = preprocessing.LabelEncoder()
le.fit(y_train)
y_train=le.transform(y_train)
le.fit(y_test)
y_test = le.transform(y_test)
print("The Encoded Data \n",y_train)


##Plot the data by showing classes separately
plot_data_for_argmax(X_train,y_train,"LPR","PEG",
                     "Showing the Training Data Classes Separately")

#Classify testing data by using SVM and Perceptron classifiers. Provide accuracies, confusion matrix and decision boundaries for both classifier
##SVM
clf_svm = make_pipeline( StandardScaler(),SVC(gamma='auto'))
clf_svm.fit(X_train, y_train)

print("Accuracy of SVM=",clf_svm.score(X_test, y_test))
###Confusion Matrix SVM
plot_cm(X_test,y_test,clf_svm,"Confusion Matrix For SVM")
###decision boundaries SVM
plot_boundries(X_test,y_test,clf_svm,xlabel='LPR',ylabel='PEG',
               title="Descion Boundries For SVM")

##Perceptron
clf_P = Perceptron(tol=1e-2, random_state=4)
clf_P.fit(X_train, y_train)

print("Accuracy For The Perceptron Classifier",clf_P.score(X_test, y_test))
###Confusion Matrix SVM
plot_cm(X_test,y_test,clf_P,"Confusion Matrix For Perceptron")
###decision boundaries SVM
plot_boundries(X_test,y_test,clf_P,xlabel='LPR',ylabel='PEG',
               title="Descion Boundries For Perceptron")

###------------------
###Problem 2
###------------------
###  Calling the function to create a classifier for each class:
###• Obtain the binarized labels (OvR) 
###• Obtain the SVM’s accuracy 
###• Plot SVM’s decision boundary
###  Return the confidence Score
###------------------
yp0 = one_vs_rest(X_train,X_test,y_train,y_test,0 )
yp1 = one_vs_rest(X_train,X_test,y_train,y_test,1 )
yp2 = one_vs_rest(X_train,X_test,y_train,y_test,2 )
yp3 = one_vs_rest(X_train,X_test,y_train,y_test,3 )

##The Overall Probability matrix
##hstack concatinates the coloum arrays
yp_all= np.hstack((yp0,yp1,yp2,yp3))
###Using argmax to get the final prediction
###argmax returns the index of the higher input value in the row
p=np.array([0,1,2,3])
y_test_aggregated_prediction=p[np.argmax(yp_all,axis=1)]
###-------------------
#Confusion Matrix For the Aggregated Performance
Classes=np.array([0,1,2,3])
plot_cm_for_argmax(y_test,y_test_aggregated_prediction,Classes,
      "Confusion Matrix For the Aggregated Performance of ovr Classifiers")
####---------------
####Data Plotting
plt.scatter(X_test[:,0][(y_test != y_test_aggregated_prediction) ],
            X_test[:,1][(y_test != y_test_aggregated_prediction) ],
            marker='D', color='red', label='Wrong Prediction')
plt.scatter(X_test[:,0][(y_test == y_test_aggregated_prediction) ],
            X_test[:,1][(y_test == y_test_aggregated_prediction) ], 
            marker='o', color='blue', label='Right Prediction')
plt.xlabel("LPR")
plt.ylabel("PEG")
plt.legend()
plt.title("Plotting Correct and Wrong Prediction Points For Aggregated OVR")
plt.show()
###----------
###Accuracy Score over the Aggregated Model of ovr
print("Accuracy Score over the Aggregated Model of ovr",
      accuracy_score(y_test, y_test_aggregated_prediction))

###----------
###Problem 3
###------------------
###  Calling the function to create a classifier for each pair of classes to:
###• Obtain the binarized labels (OvO) 
###• Obtain the SVM’s accuracy (0.5 Marks)
###• Plot SVM’s decision boundary
###  Return the confidence Score
###------------------
 
yp01_0=one_vs_one_RETURN_PROBABILITY_FOR_ALL(X_train,X_test,y_train,y_test,0,1 )
yp02_0=one_vs_one_RETURN_PROBABILITY_FOR_ALL(X_train,X_test,y_train,y_test,0,2 )
yp03_0=one_vs_one_RETURN_PROBABILITY_FOR_ALL(X_train,X_test,y_train,y_test,0,3 )
yp12_0=one_vs_one_RETURN_PROBABILITY_FOR_ALL(X_train,X_test,y_train,y_test,1,2 )
yp13_0=one_vs_one_RETURN_PROBABILITY_FOR_ALL(X_train,X_test,y_train,y_test,1,3 )
yp23_0=one_vs_one_RETURN_PROBABILITY_FOR_ALL(X_train,X_test,y_train,y_test,2,3 )
####------------
#### We Have six classifiers Each Has its own prediction for Each point in the data to be one of two classes((0,1),(0,2)......(2,3)) with a certain confidences
#### we will get the sum of those confidences for the same class for the same point 
#### the class with the higher summation will be the winner the final predict
####---------


yp0=np.array([yp01_0[:,0]+yp02_0[:,0]+yp03_0[:,0]]).reshape(-1,1)
yp1=np.array([yp01_0[:,1]+yp12_0[:,0]+yp13_0[:,0]]).reshape(-1,1)
yp2=np.array([yp02_0[:,1]+yp12_0[:,1]+yp23_0[:,0]]).reshape(-1,1)
yp3=np.array([yp03_0[:,1]+yp13_0[:,1]+yp23_0[:,1]]).reshape(-1,1)
yp_all_2=  np.hstack([yp0,yp1,yp2,yp3])

###Using argmax to get the final prediction
Classes=np.array([0,1,2,3])
y_test_aggregated_prediction_1=Classes[np.argmax(yp_all_2,axis=1)]

###-------------------
#Confusion Matrix For the Aggregated Performance
le = preprocessing.LabelEncoder()
le.fit(y_test)
y_test = le.transform(y_test)

plot_cm_for_argmax(y_test,y_test_aggregated_prediction_1,Classes,
        "Confusion Matrix For the Aggregated Performance of ovo Classifiers")

####---------------
####Data Plotting

#plot_data_for_argmax(X_test,y_test_aggregated_prediction_1,"LPR","PEG","Overall Predicted Data Plotting ")
#plot_data_for_argmax(X_test,y_test,"LPR","PEG","Actual Data Plotting_1")
plt.scatter(X_test[:,0][(y_test != y_test_aggregated_prediction_1) ],
            X_test[:,1][(y_test != y_test_aggregated_prediction_1) ],
            marker='D', color='red', label='Wrong Prediction')
plt.scatter(X_test[:,0][(y_test == y_test_aggregated_prediction_1) ], 
            X_test[:,1][(y_test == y_test_aggregated_prediction_1) ],
            marker='o', color='blue', label='Right Prediction')
plt.xlabel("LPR")
plt.ylabel("PEG")
plt.legend()
plt.title("Plotting Correct and Wrong Prediction Points For Aggregated OVO")
plt.show()
###----------
###Accuracy Score over the Aggregated Model of ovo
print("Accuracy Score over the Aggregated Model of ovo",
      accuracy_score(y_test, y_test_aggregated_prediction_1))

